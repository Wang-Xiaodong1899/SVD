{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/src')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.splits import create_splits_scenes\n",
    "from nuscenes.can_bus.can_bus_api import NuScenesCanBus\n",
    "from einops import rearrange, repeat\n",
    "from PIL import Image\n",
    "import fire\n",
    "from safetensors import safe_open\n",
    "\n",
    "from common import json2data\n",
    "\n",
    "from diffusers.utils import export_to_video\n",
    "\n",
    "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection, CLIPModel\n",
    "from diffuser.models.unet_action_v11 import UNetSpatioTemporalConditionModel_Action\n",
    "# from diffusers.models.unet_action import UNetSpatioTemporalConditionModel_Action\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffuser.pipelines.stable_video_diffusion.pipeline_action_video_diffusion_v1_v_imclip import ActionVideoDiffusionPipeline\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(pretrained_model_name_or_path = '/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels-vis/ti2v_s448_imclip/checkpoint-16000', device='cuda:0'):\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "                '/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/image-allframes-ep100-checkpoint-30000', subfolder=\"text_encoder\",\n",
    "    )\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "                '/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/image-allframes-ep100-checkpoint-30000', subfolder=\"vae\",\n",
    "    )\n",
    "    clip_model = CLIPModel.from_pretrained(\n",
    "        \"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "\n",
    "    text_encoder.eval()\n",
    "    vae.eval()\n",
    "    clip_model.eval()\n",
    "    text_encoder.to(device)\n",
    "    vae.to(device)\n",
    "    clip_model.to(device)\n",
    "\n",
    "    tokenizer = CLIPTokenizer.from_pretrained('/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/image-allframes-ep100-checkpoint-30000', subfolder=\"tokenizer\")\n",
    "    scheduler = DDPMScheduler.from_pretrained('/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/image-allframes-ep100-checkpoint-30000', subfolder=\"scheduler\")\n",
    "    feature_extractor = CLIPImageProcessor.from_pretrained('/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/image-allframes-ep100-checkpoint-30000', subfolder=\"feature_extractor\")\n",
    "\n",
    "    # unet = UNetSpatioTemporalConditionModel_Action.from_pretrained(pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "    # unet.temp_style = \"image\" # use image clip embedding\n",
    "\n",
    "    unet = UNetSpatioTemporalConditionModel_Action(cross_attention_dim=768, in_channels=4, temp_style = \"image\")\n",
    "    if \"unet\" not in os.listdir(pretrained_model_name_or_path):\n",
    "        dm_path = os.path.join(pretrained_model_name_or_path, \"diffusion_pytorch_model.safetensors\")\n",
    "    else:\n",
    "        dm_path = os.path.join(pretrained_model_name_or_path, \"unet\", \"diffusion_pytorch_model.safetensors\")\n",
    "    tensors = {}\n",
    "    with safe_open(dm_path, framework=\"pt\", device='cpu') as f:\n",
    "        for k in f.keys():\n",
    "            tensors[k] = f.get_tensor(k)\n",
    "    miss_keys, ignore_keys = unet.load_state_dict(tensors, strict=True)\n",
    "    print(f'loaded unet from {pretrained_model_name_or_path}')\n",
    "    print('miss_keys: ', miss_keys)\n",
    "    print('ignore_keys: ', ignore_keys)\n",
    "\n",
    "    unet.eval()\n",
    "    unet = unet.to(device)\n",
    "    pipeline = ActionVideoDiffusionPipeline(\n",
    "            text_encoder=text_encoder,\n",
    "            vae=vae,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            tokenizer=tokenizer,\n",
    "            feature_extractor=feature_extractor,\n",
    "            clip_model=clip_model\n",
    "    )\n",
    "    pipeline = pipeline.to(device).to(torch.float16)\n",
    "    return pipeline\n",
    "\n",
    "def generate_caption(image, git_processor_large, git_model_large, device='cuda:0'):\n",
    "\n",
    "    git_model_large.to(device)\n",
    "    pil_list = [image.convert('RGB')]\n",
    "    inputs = git_processor_large(images=pil_list, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = git_model_large.generate(pixel_values=inputs.pixel_values, max_length=50)\n",
    "\n",
    "    generated_caption = git_processor_large.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "   \n",
    "    return generated_caption[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = '/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels-vis/ti2v_s448_imclip/checkpoint-16000'\n",
    "pipeline = load_models(pretrained_model_name_or_path, \"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_processor_large = AutoProcessor.from_pretrained(\"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/git-large-coco\")\n",
    "git_model_large = AutoModelForCausalLM.from_pretrained(\"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels/git-large-coco\")\n",
    "print('loaded caption model!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 36\n",
    "train_frames = 8\n",
    "device='cuda'\n",
    "roll_out= 4\n",
    "width = 448\n",
    "height = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 12):\n",
    "    image_path = f\"/mnt/storage/user/wangxiaodong/framework/DWM/waymo_{i}.jpg\"\n",
    "\n",
    "    root_dir = \"waymo_case\"\n",
    "    image = Image.open(image_path)\n",
    "    prompt = generate_caption(image, git_processor_large, git_model_large)\n",
    "\n",
    "    video = pipeline(image, num_frames=train_frames, prompt=prompt, action=None, height=height, width=width).frames[0]\n",
    "    # replace first frame\n",
    "    video[0] = image.resize((width, height))\n",
    "    for t in range(roll_out):\n",
    "        last_frame = video[-1]\n",
    "        prompt = generate_caption(last_frame, git_processor_large, git_model_large)\n",
    "        video_2 = pipeline(last_frame, num_frames=train_frames, prompt=prompt, action=None, height=height, width=width).frames[0]\n",
    "        video = video + video_2[1:]\n",
    "\n",
    "    print(f'len of final video {len(video)}')\n",
    "\n",
    "    name = os.path.basename(image_path).split('.')[0]\n",
    "\n",
    "    os.makedirs(os.path.join(root_dir), exist_ok=True)\n",
    "\n",
    "    export_to_video(video, os.path.join(root_dir, f'{name}.mp4'), fps=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
