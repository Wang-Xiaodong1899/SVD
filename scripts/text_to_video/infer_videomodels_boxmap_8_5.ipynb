{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d04d3794-0fe1-4827-88f2-beae1629f7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 12:13:19.548362: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-07 12:13:19.747708: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-07 12:13:20.344262: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-07 12:13:23.383512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/root/SVD/src/diffuser/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "from diffusers.utils import export_to_video\n",
    "sys.path.append('/root/SVD/src')\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPImageProcessor\n",
    "from diffuser.models.unet_action_v11 import UNetSpatioTemporalConditionModel_Action\n",
    "from diffuser import StableDiffusionPipeline, AutoencoderKL, DDIMScheduler, UNet2DConditionModel\n",
    "from safetensors import safe_open\n",
    "from diffuser.pipelines.stable_video_diffusion.pipeline_action_video_diffusion_v1_v_imclip import ActionVideoDiffusionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64a9c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add image context parameters\n",
      "down & mid context in channel: 320\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "add image context parameters\n",
      "down & mid context in channel: 320\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "add image context parameters\n",
      "down & mid context in channel: 640\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "add image context parameters\n",
      "down & mid context in channel: 1280\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "add image context parameters\n",
      "down & mid context in channel: 1280\n",
      "add image context parameters\n",
      "up block context in channels: 1280\n",
      "add image context parameters\n",
      "up block context in channels: 640\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "add image context parameters\n",
      "up block context in channels: 320\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "add image context parameters\n",
      "up block context in channels: 320\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "cross_attention_dim: 1024\n",
      "init temp style: image\n",
      "loaded unet from /mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels-vis/ti2v_s448_imclip_boxmap_allframe/checkpoint-44000\n",
      "miss_keys:  []\n",
      "ignore_keys:  []\n"
     ]
    }
   ],
   "source": [
    "# prepare components\n",
    "pretrained_model_name_or_path = \"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/smodels-vis/ti2v_s448_imclip_boxmap_allframe/checkpoint-44000\"\n",
    "device = 'cuda:0'\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    '/mnt/storage/user/wuzehuan/Downloads/models/stable-diffusion-2-1', subfolder=\"text_encoder\", variant=\"fp16\"\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    '/mnt/storage/user/wuzehuan/Downloads/models/stable-diffusion-2-1', subfolder=\"vae\"\n",
    ")\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "    \"/mnt/storage/user/wuzehuan/Downloads/models/CLIP-ViT-H-14-laion2B-s32B-b79K\", torch_dtype=torch.float16)\n",
    "\n",
    "unet = UNetSpatioTemporalConditionModel_Action(cross_attention_dim=1024, in_channels=4, temp_style = \"image\")\n",
    "if \"unet\" not in os.listdir(pretrained_model_name_or_path):\n",
    "    dm_path = os.path.join(pretrained_model_name_or_path, \"diffusion_pytorch_model.safetensors\")\n",
    "else:\n",
    "    dm_path = os.path.join(pretrained_model_name_or_path, \"unet\", \"diffusion_pytorch_model.safetensors\")\n",
    "tensors = {}\n",
    "with safe_open(dm_path, framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "miss_keys, ignore_keys = unet.load_state_dict(tensors, strict=True)\n",
    "print(f'loaded unet from {pretrained_model_name_or_path}')\n",
    "print('miss_keys: ', miss_keys)\n",
    "print('ignore_keys: ', ignore_keys)\n",
    "text_encoder.eval()\n",
    "vae.eval()\n",
    "clip_model.eval()\n",
    "text_encoder.to(device)\n",
    "vae.to(device)\n",
    "clip_model.to(device)\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained('/mnt/storage/user/wuzehuan/Downloads/models/stable-diffusion-2-1', subfolder=\"tokenizer\")\n",
    "scheduler = DDIMScheduler.from_pretrained('/mnt/storage/user/wuzehuan/Downloads/models/stable-diffusion-2-1', subfolder=\"scheduler\")\n",
    "feature_extractor = CLIPImageProcessor.from_pretrained('/mnt/storage/user/wuzehuan/Downloads/models/stable-diffusion-2-1', subfolder=\"feature_extractor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf14f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scheduler prediction type: v_prediction\n"
     ]
    }
   ],
   "source": [
    "unet.eval()\n",
    "unet = unet.to(device)\n",
    "pipeline = ActionVideoDiffusionPipeline(\n",
    "        text_encoder=text_encoder,\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "        scheduler=scheduler,\n",
    "        tokenizer=tokenizer,\n",
    "        feature_extractor=feature_extractor,\n",
    "        clip_model=clip_model\n",
    ")\n",
    "pipeline = pipeline.to(device).to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b28f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "layout_encoder = timm.create_model(model_name=\"convnextv2_base.fcmae\", pretrained=True, num_classes=0)\n",
    "layout_encoder.to(device)\n",
    "layout_encoder = layout_encoder.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3659f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image processer\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 448)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e31b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "utime = f'{1532402941762460}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb62a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "_3dbox_image_path = f\"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/mini_3dbox_hdmap/3dbox_{utime}.jpg\"\n",
    "hdmap_image_path = f\"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/mini_3dbox_hdmap/hdmap_{utime}.jpg\"\n",
    "_3dbox_image = Image.open(_3dbox_image_path)\n",
    "hdmap_image = Image.open(hdmap_image_path)\n",
    "_3dbox = transform(_3dbox_image)\n",
    "hdmap = transform(hdmap_image)\n",
    "_3dbox = _3dbox.unsqueeze(0).to(device)\n",
    "hdmap = hdmap.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56fba73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = f\"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/mini_3dbox_hdmap/{utime}.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "with open(f\"/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/mini_3dbox_hdmap/caption_{utime}.txt\", 'r', encoding='utf-8') as file:\n",
    "    caption = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff2ddfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 112, 1024])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dtype = torch.float16\n",
    "_3dbox_embeddings = layout_encoder\\\n",
    "    .forward_features(_3dbox.to(weight_dtype))\\\n",
    "    .flatten(-2).permute(0, 2, 1)\n",
    "hdmap_embeddings = layout_encoder\\\n",
    "    .forward_features(hdmap.to(weight_dtype))\\\n",
    "    .flatten(-2).permute(0, 2, 1)\n",
    "hdmap_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b412c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the car is parked on the road.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# provide prompt_embeds\n",
    "print(caption)\n",
    "inputs = tokenizer(\n",
    "    caption, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "text_input = inputs['input_ids'].to(device)\n",
    "encoder_hidden_states = text_encoder(text_input)[0]\n",
    "encoder_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc403e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final encoder hidden states\n",
    "encoder_hidden_states = torch.cat([encoder_hidden_states, _3dbox_embeddings, hdmap_embeddings], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd46a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image clip embedding torch.Size([1, 1, 1024])\n",
      "text emb shape:  torch.Size([1, 301, 1024])\n",
      "no noise add to image\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b815832008e445639d9bd9869c71fb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = pipeline(image=image, num_frames=8, action=None, prompt_embeds=encoder_hidden_states, height=256, width=448).frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac90c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug/utime_1532402941762460.mp4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_to_video(video, os.path.join('/mnt/storage/user/wangxiaodong/DWM_work_dir/lidar_maskgit_debug', f'utime_{utime}.mp4'), fps=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb1626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
